<div class="container" id="i18_discussion">

    <div class="row">
        <div class="col-lg-8 col-lg-offset-2 text-left m-t-lg m-b-lg wow zoomIn">
            <div class="col-lg-12 text-center">
                <h1><a id="user-content-discussing-recording-and-playback" class="anchor" aria-hidden="true"
                       href="#discussing-recording-and-playback"><span style="font-size: 65%">ðŸ”—</span></a> Discussing
                    Recording and Playback</h1>
            </div>
            <h2><a id="user-content-recording-a-http-conversation" class="anchor" aria-hidden="true"
                   href="#recording-a-http-conversation"><span style="font-size: 65%">ðŸ”—</span></a> Recording a HTTP
                conversation</h2>
            <p>You'll write your test (say JUnit) and that will use a library (that your company may have written or be
                from a vendor). For
                recording you will swap the real service URL for one running a Servirtium middle-man server (which
                itself will delegate to
                the real service). If that service is flaky - keep re-running the test manually until the service is
                non-flaky, and commit
                that Servirtium-style markdown to source-control. Best practice is to configure the same test two have
                two modes of operation: 'direct' and 'recording' modes.  This is <strong>not</strong> a caching idea
                - it is deliberate
                - you are explicitly recording while running a test, or not recording while running a test (and doing
                direct to the service)</p>
            <p>Anyway, the recording ends up in the markdown described in a text file on your file system - which you'll
                commit to VCS alongside your tests.</p>
            <h2><a id="user-content-playback-of-http-conversations" class="anchor" aria-hidden="true"
                   href="#playback-of-http-conversations"><span style="font-size: 65%">ðŸ”—</span></a> Playback of HTTP
                conversations</h2>
            <p>Those same markdown recordings are used in playback. Again an explicit mode - you're running in this mode
                and it will fail if there are no recordings in the dir/file in source control.</p>
            <p>Playback itself will fail if the headers/body <strong>sent by the client to the real service</strong>
                (through the Servirtium library)
                are <strong>not</strong> the same they were when the recording was made. It is possible that
                masking/redacting and general manipulations should happen
                deliberately during the recording to get rid of transient aspects that are not helpful in playback
                situations. The test
                failing in this situation is deliberate - you're using this to guard against potential
                incompatibilities.</p>
            <p>For example any dates in headers of the body that go from the client to the HTTP Server could be swapped
                for some date
                in the future like "2099-01-01" or a date in the past "1970-01-01".</p>
            <p>The person who's designing the tests
                that recording or playback would work on the redactions/masking towards an "always passing" outcome,
                with no differences
                in the markdown regardless of the number of time the same test is re-recorded.</p>
            <p>Note: How a difference in request-header or request-body expectation is
                logged in the test output needs to be part of the deliberate design of the tests themselves. This is
                easier said than
                done, and you can't catch assertion failures over HTTP.</p>
            <p>Note2: this is a third mode of operation for the same test as in "Recording a HTTP conversation" above -
                "playback"
                mode meaning you have three modes of operation all in all.</p>
        </div>
    </div>
</div>
